---
title: "Surface Detection by Robot Movements - R Script"
author: "Marian Dumitrascu"
date: "March 31, 2019"
output: 
  pdf_document:
    fig_cap: yes
    keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The R Script 

For this project I choose a Kaggle.com open competition project. This is [*CareerCon 2019 - Help Navigate Robots*](https://www.kaggle.com/c/career-con-2019). 

This document is the R Script that uses the final model described in the report for predicting the surface a robot is moving, based on data from three sensors: inertial, magnetostatic and gyroscopic.
Data is downloaded from a AWS S3 bucket that I prepared for the duration of grading of this project.
This data together with an intermediarry set of data is stored in a subfolder *data*

The script uses the full training dataset to produce a set of 9 models one for each surface type that are saved on hard-disk in a subfolder *models*.
At the end it will run on the full test dataset and create a file in the format accepted by Kaggle for submission. 

I also keep this project on GitHub: https://github.com/mariandumitrascu/ph125_9_HelpRobotsNavigate

Running this script could take considerable amount of time and require at least 8Gb of RAM. 


```{r load packages, warning=TRUE, include=FALSE}
options(repos="https://CRAN.R-project.org")

# install.packages("ISLR")
# install.packages("orientlib")
# install.packages("RSpincalc")
# devtools::install_github("collectivemedia/tictoc")
# install.packages("kableExtra")
# install.packages("doParallel", dependencies = TRUE)
# install.packages("randomForest", dependencies = TRUE)
# install.packages("corrplot")

library(tidyverse)
library(readr)
library(dplyr)
library(ISLR)
library(caret)
library(orientlib)
library(matrixStats)
library(randomForest)
library(RSpincalc)
library(tictoc)
library(corrplot)

# #######################################################################################################
# data loading

# load X_train.csv and y_train.csv 
x_train <- read_csv("https://s3.amazonaws.com/terraform-bucket-dq001/X_train.csv", col_names = TRUE)
y_train <- read_csv("https://s3.amazonaws.com/terraform-bucket-dq001/y_train.csv", col_names = TRUE)
x_test <- read_csv("https://s3.amazonaws.com/terraform-bucket-dq001/X_test.csv", col_names = TRUE)

# x_train <- read_csv("data/X_train.csv")
# y_train <- read_csv("data/y_train.csv")
# x_test <- read_csv("data/x_test.csv")

# then join them
x_train <- x_train %>% inner_join(y_train, by = "series_id")

# convert surface to factor
x_train <- mutate(x_train, surface = as.factor(surface))


# #######################################################################################################
# pre-processing - convert quaternions to euler angles

# define a function to convert quaternion values to euler angles. 
convert_quaternions_to_euler <- function(a_dataset){
	
	# use Q2EA from RSpincalc to convert quaternions to euler angles
	Q <- a_dataset %>% select(
					orientation_X, 
					orientation_Y, 
					orientation_Z, 
					orientation_W) %>% 
				as.matrix()
	
	euler_matrix <- Q2EA(Q, 
				 EulerOrder='xyz', 
				 tol = 10 * .Machine$double.eps, 
				 ichk = FALSE, 
				 ignoreAllChk = FALSE)

	# add the new columns to the dataset
	a_dataset <- a_dataset %>% mutate(
					phi = euler_matrix[,1],
					theta = euler_matrix[,2], 
					psi = euler_matrix[,3])
	
	# remove quaternion columns
	a_dataset <- a_dataset %>% select(
				-orientation_X, 
				-orientation_Y,  
				-orientation_Z, 
				-orientation_W)
	
	# return the new dataset
	a_dataset
}

x_train <- convert_quaternions_to_euler(x_train)
x_test <- convert_quaternions_to_euler(x_test)

# #######################################################################################################
# pre-processing - creating features

# n_of_rows defaults to total number ofseries
# is_train indicates that the data is training, thus will do an extra action
pre_process <- function(a_dataframe, n_of_rows = nrow(a_dataframe)/128 ) {
	
	# get data grouped by seeries_id and compute some means 
	processed_data_df <- a_dataframe %>% 
	group_by(series_id) %>% 
	summarize(
		phi_mean_all = mean(phi),
		phi_sd_all = sd(phi),
		phi_mean_to_sd_all = mean(phi)/sd(phi),
		theta_mean_all = mean(theta),
		theta_sd_all = sd(theta),
		theta_mean_to_sd_all = mean(theta)/sd(theta),
		psi_mean_all = mean(psi),
		psi_sd_all = sd(psi),
		psi_mean_to_sd_all = mean(psi)/sd(psi),
		# this is the rectangular area that surounds the path of linear movement
		dist_area = (max(linear_acceleration_X) - min(linear_acceleration_X)) * (max(linear_acceleration_Y) - min(linear_acceleration_Y)) + 
			(max(linear_acceleration_X) - min(linear_acceleration_X)) * (max(linear_acceleration_Z) - min(linear_acceleration_Z)) + 
			(max(linear_acceleration_Y) - min(linear_acceleration_Y)) * (max(linear_acceleration_Z) - min(linear_acceleration_Z)),
		# this is the rectangular area that surounds the path of angular movement
		omega_area = (max(angular_velocity_X) - min(angular_velocity_X)) * (max(angular_velocity_Y) - min(angular_velocity_Y)) +
			(max(angular_velocity_X) - min(angular_velocity_X)) * (max(angular_velocity_Z) - min(angular_velocity_Z)) +
			(max(angular_velocity_Y) - min(angular_velocity_Y)) * (max(angular_velocity_Z) - min(angular_velocity_Z)),
		euler_area = (max(phi) - min(phi)) * (max(theta) - min(theta)) + 
			(max(phi) - min(phi)) * (max(psi) - min(psi)) + 
			(max(theta) - min(theta)) * (max(psi) - min(psi)),
		dist_mean_x = mean(linear_acceleration_X),
		dist_mean_y = mean(linear_acceleration_Y),
		dist_mean_z = mean(linear_acceleration_Z),
		omega_mean_x = mean(angular_velocity_X),
		omega_mean_y = mean(angular_velocity_Y),
		omega_mean_Z = mean(angular_velocity_Z),
		dist_sd_x = sd(linear_acceleration_X),
		dist_sd_y = sd(linear_acceleration_Y),
		dist_sd_z = sd(linear_acceleration_Z),
		omega_sd_x = sd(angular_velocity_X),
		omega_sd_y = sd(angular_velocity_Y),
		omega_sd_Z = sd(angular_velocity_Z)	
		
		) %>% 
		slice(1:n_of_rows)
	
	# define an empty data frame with summary metrics that we'll use for each set of 128 observations
	metrics <- c("dist_total","dist_max","dist_min","dist_max_to_min","dist_mean","dist_sd","dist_mean_to_sd",
							 "omega_total","omega_max","omega_min","omega_max_to_min","omega_mean","omega_sd","omega_mean_to_sd",
							 "phi_total","phi_max","phi_min","phi_mean","phi_sd","phi_mean_to_sd",
							 "theta_total","theta_max","theta_min","theta_mean","theta_sd","theta_mean_to_sd",
							 "psi_total","psi_max","psi_min","psi_mean","psi_sd","psi_mean_to_sd",
							 "euler_total","euler_max","euler_min","euler_mean","euler_sd","euler_mean_to_sd")
	tmp_df <- data.frame(matrix(ncol = length(metrics), nrow = 0) )
	colnames(tmp_df) <- metrics

	# loop over each series and compute aggegations
	# should use apply type of function here, but I use "for" until I master the apply
	for (s_id in processed_data_df$series_id)
	{
		# get current measurement set
		this_chunk_df <- a_dataframe %>% filter(series_id == s_id)
		
		# select only columns we are interested in 
		this_chunk_df <- this_chunk_df %>% 
			select(
			phi, theta, psi,  
			angular_velocity_X, angular_velocity_Y, angular_velocity_Z,
			linear_acceleration_X, linear_acceleration_Y, linear_acceleration_Z)
		# this is a vector with euclidian distances from one point to the next
		dist_v <- 	sqrt(diff(this_chunk_df$linear_acceleration_X)^2 + 
								diff(this_chunk_df$linear_acceleration_Y)^2 + 
								diff(this_chunk_df$linear_acceleration_Z)^2)
		omega_v <- 	sqrt(diff(this_chunk_df$angular_velocity_X)^2 + 
								diff(this_chunk_df$angular_velocity_Y)^2 + 
								diff(this_chunk_df$angular_velocity_Z)^2)
		phi_v <- abs(diff(this_chunk_df$phi))
		theta_v <- abs(diff(this_chunk_df$theta))
		psi_v <- abs(diff(this_chunk_df$psi))

		# fill or temp data frame with summary computations for our 128 measurement set
		tmp_df <- bind_rows(tmp_df, data_frame(
			
			# all features starting with "dist_" refers to linear movement
			dist_total = sum(dist_v),
			dist_max = max(dist_v),
			dist_min = min(dist_v),
			dist_max_to_min = max(dist_v)/min(dist_v),
			dist_mean = mean(dist_v),
			dist_sd = sd(dist_v),
			dist_mean_to_sd = mean(dist_v)/sd(dist_v),  # reciprocal coef of variation
			
			# all features starting with "omega_" refers to angle velocity measurments
			omega_total = sum(omega_v),
			omega_max = max(omega_v),
			omega_min = min(omega_v),
			omega_max_to_min = max(omega_v)/min(omega_v),
			omega_mean = mean(omega_v),
			omega_sd = sd(omega_v),
			omega_mean_to_sd = mean(omega_v)/sd(omega_v), # reciprocal coef of variation

			# phi, theta and psi reffers to roll, pitch and yaw rotations
			phi_total = sum(phi_v),
			phi_max = max(phi_v),
			phi_min = min(phi_v),
			phi_mean = mean(phi_v),
			phi_sd = sd(phi_v),
			phi_mean_to_sd = mean(phi_v)/sd(phi_v),
			
			theta_total = sum(theta_v),
			theta_max = max(theta_v),
			theta_min = min(theta_v),
			theta_mean = mean(theta_v),
			theta_sd = sd(theta_v),
			theta_mean_to_sd = mean(theta_v)/sd(theta_v),
			
			psi_total = sum(psi_v),
			psi_max = max(psi_v),
			psi_min = min(psi_v),
			psi_mean = mean(psi_v),
			psi_sd = sd(psi_v),
			psi_mean_to_sd = mean(psi_v)/sd(psi_v),
			
			# features starting with "euler_" refers to 
			# agragation of all phi, theta and psi rotations
			euler_total = sum(phi_v + theta_v + psi_v), 
			euler_max = max(phi_v + theta_v + psi_v),
			euler_min = min(phi_v + theta_v + psi_v),
			euler_mean = mean(phi_v + theta_v + psi_v),
			euler_sd = sd(phi_v + theta_v + psi_v),
			euler_mean_to_sd = mean(phi_v + theta_v + psi_v)/sd(phi_v + theta_v + psi_v)
			))
	
	} # end of for over series
	
	# add the summary computations to the data set of series
	processed_data_df <- bind_cols(processed_data_df, tmp_df)

	# more features
	# I added thesee as an experimentation after observing the dependencies graphs
	# will explain later
	processed_data_df <- processed_data_df %>% mutate(
		# this is an agular momentum 
		f1 = log(dist_mean_to_sd*omega_mean_to_sd),
		
		# this is an angular momentum 
		f2 = log(dist_total*omega_total),
		
		# this is the angle between theta and psi vectors
		f3 = abs(atan(theta_mean_all/psi_mean_all)))		
	
	# return the proceessed data
	processed_data_df
}


# pre-process train and test data sets
tic("processing x_train data")
x_train_processed <- pre_process(x_train)
toc()

tic("processing x_test data")
x_test_processed <- pre_process(x_test)
toc()

# rejoin train data with the labels data set
x_train_processed <- x_train_processed %>% left_join(y_train, by = "series_id")

# create a folder "data" if doesnt exist
if (!dir.exists("data")) dir.create("data")

# save processed data, and use these files from now on
write_csv(x_train_processed, "data/x_train_processed.csv")
write_csv(x_test_processed, "data/x_test_processed.csv")

# clean some variables and the environment
rm(x_train, x_test, y_train)
rm(x_train_processed, x_test_processed)
```


```{r pree-processing and modeling}

# #######################################################################################################
# load pre-processed data from file

x_train_processed_from_file <- read_csv("data/x_train_processed.csv")
x_test_processed_from_file <- read_csv("data/x_test_processed.csv")

# if we load data from a file, convert surface to factor
x_train_processed_from_file <- x_train_processed_from_file %>% mutate(surface = as.factor(surface))


x_test_processed <- x_test_processed_from_file
x_train_processed <- x_train_processed_from_file 

# #######################################################################################################
#  pre-processing - feature selection

# pre-process the data, center and scale the values across all predictors
pre_process <- x_train_processed %>% select(-series_id, -group_id) %>% preProcess(method = c("center", "scale"))
x_train_processed <- predict(pre_process, x_train_processed)
x_test_processed <- predict(pre_process, x_test_processed)

rm(pre_process)


# convert both test and train data to matrix in order to analyse feature corelation
x_train_matrix <- x_train_processed %>% select(-surface, -series_id) %>% as.matrix()
x_test_matrix <- x_test_processed %>% select(-series_id) %>% as.matrix()

# find features that are high correlated 
# find linear dependencies and eliminate them
names_to_remove_test <- findCorrelation(cor(x_test_matrix), cutoff = 0.95, names = TRUE, verbose = FALSE, exact=TRUE)

# remove correlated features from both train and test sets
x_train_processed <- x_train_processed %>% select(-names_to_remove_test) 
x_test_processed <- x_test_processed %>% select(-names_to_remove_test) 


# remove columns do not contribute to classification
x_train_processed <- x_train_processed %>% select(-theta_min, -omega_max_to_min, -dist_mean_y, -omega_mean_x, -dist_mean_x, -dist_mean_z)
x_test_processed <- x_test_processed %>% select(-theta_min, -omega_max_to_min, -dist_mean_y, -omega_mean_x, -dist_mean_x, -dist_mean_z)


# #######################################################################################################
# randomForest model one-vs-one training

# store the train data in a new variable
x_train_processed_ova <- x_train_processed 

# a prefix to save models on file system
model_prefix <- "model_15_fit_"

# create a subfolder called "models if it doesnt exists"
if (!dir.exists("models")) dir.create("models")

# partition data into:train, test, and balancing pool
# we will use the pool to extract records to balance the dataset
folds <- createFolds(x_train_processed_ova$surface, k = 3, list = TRUE)
x_train_for_train_ova <- x_train_processed_ova[folds$Fold1,]
x_train_for_test_ova <- x_train_processed_ova[folds$Fold2,]
x_train_pool <- x_train_processed_ova[folds$Fold3,]

# get surfaces in a data frame, so we can loop over
surfaces <- x_train_for_train_ova %>% group_by(surface) %>% 
	summarize(n = n()) %>% 
	mutate(surface = as.character(surface)) %>% 
	# filter(surface == "hard_tiles") %>% 
	arrange(n)

# idealy, I should use apply function but I'm still working on that
# this can bee also be improved if I would use foreacch packade with %dopar% option for parallelization.,
# still work in progress
# this could take more than 1 hour
for(current_surface in surfaces$surface)
{
		tic(paste("generating model for:"), current_surface)
	
		# convert surface to two values: current surface and "the_rest"
		x_train_for_train_ova_current <- x_train_for_train_ova %>% 
			mutate(surface = ifelse(surface == current_surface, current_surface, "the_rest")) %>% 
			mutate(surface = as.factor(surface))
		
		# add records from the pool to balance the recordset
		x_chunk_for_balance <- x_train_pool %>% filter(surface == current_surface)
		x_train_for_train_ova_current <- bind_rows(x_train_for_train_ova_current, x_chunk_for_balance)
		
		# ##################################################################################################
		# custom randomForest
		mtry <- sqrt(ncol(x_train_for_train_ova_current) - 1)
		tunegrid <- expand.grid(.mtry=mtry,.ntree=c( 300,500,1000, 1500))
		control <- trainControl(method="repeatedcv", 
														number=10, 
														repeats=2, 
														search="grid", 
														classProbs = TRUE,
														# we could also use subsampling, but this will make it run even slower
														sampling = "up",
														summaryFunction = twoClassSummary
														)
		customRF 						<- 	list(type = "Classification", library = "randomForest", loop = NULL)
		customRF$parameters <- 	data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
		customRF$grid 			<- 	function(x, y, len = NULL, search = "grid") {}
		customRF$fit 				<- 	function(x, y, wts, param, lev, last, weights, classProbs, ...) randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
		customRF$predict 		<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL) predict(modelFit, newdata)
		customRF$prob 			<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL)	predict(modelFit, newdata, type = "prob")
		customRF$sort 			<- 	function(x) x[order(x[,1]),]
		customRF$levels 		<- 	function(x) x$surface
				
		model_fit_current <- train(surface ~ ., 
															 data = select(x_train_for_train_ova_current, -series_id, -group_id), 
															 method=customRF, 
															 # use ROC for the metric because Accuracy is not the best 
															 # in case of this heavy unballanced data seet
															 metric="ROC", 
															 tuneGrid=tunegrid, 
															 trControl=control)
		# ##################################################################################################
		# save the model into /models folder
		model_name <- paste(model_prefix, current_surface, sep = "")
		file <- paste("models/",  model_name, ".rds", sep = "")
		write_rds(model_fit_current, file)
		
		toc()
}


# #######################################################################################################
# load the models and perform model prediction and evaluation using test data split from training:


# # create a data frame the will store probabilities for each model
# we'll use this for voting
# the model with highes prediction will get the vote
results_voting <- data.frame(
	series_id = x_train_for_test_ova$series_id, 
	true_surface = x_train_for_test_ova$surface)

for(current_surface in surfaces$surface) {
	
	# prepare the test dataset: we keep current surface name, and we rename all other surfaces to "the_rest"
	# we have now a binary clasification.
	x_train_for_test_ova_current <- x_train_for_test_ova %>% 
			mutate(surface = ifelse(surface == current_surface, current_surface, "the_rest")) %>% 
			mutate(surface = as.factor(surface))
	
	# get the model from a file
	model_name <- paste(model_prefix, current_surface, sep = "")
	model_fit_current <- readRDS(paste("models/", model_name, ".rds", sep = ""))
	
	# get y_hat_prob
	y_hat_prob <- predict(
										model_fit_current, 
										select(x_train_for_test_ova_current, -series_id), 
										type = "prob")
	
	# store the probability of curent model for current surface in a column named by current surface
	results_voting <- results_voting %>% mutate(last_result_prob = y_hat_prob[,current_surface])
	names(results_voting)[ncol(results_voting)] <- current_surface # the column name is current surface

}

# add an empty column for predicted surfaces 
results_voting <- results_voting %>%  mutate(pred_surface = rep("", nrow(results_voting)))

# set the value on predicted surface to the surface that got maximum probability
for (i in 1:nrow(results_voting)) {
		results_voting[i, "pred_surface"] <- names(which.max(select(results_voting[i,], -series_id, -true_surface, -pred_surface)))
}

results_voting <- results_voting %>% mutate(pred_surface = as.factor(pred_surface))


# compute confusion matrix and print it
conf_matrix <- confusionMatrix(results_voting$pred_surface,
															 results_voting$true_surface)

# display confusion matrix
conf_matrix$table %>% knitr::kable()

# create a data frame to store Accuracy results by model
model_results <- data.frame(Model = "randomForest one-vs-one", Accuracy = conf_matrix$overall["Accuracy"])
model_results %>% knitr::kable()




```

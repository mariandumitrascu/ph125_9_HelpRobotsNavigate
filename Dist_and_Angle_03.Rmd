---
title: "Surface Detection by Robot Movements"
author: "Marian Dumitrascu"
date: "March 19, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Surface Detection by Robot Movements


## Introduction

gyros, accelerometer and magnetometer sensor

## Load Libraries

```{r data load, message=FALSE, warning=FALSE}
options(repos="https://CRAN.R-project.org")
# install.packages("ISLR")
# install.packages("orientlib")
# install.packages("RSpincalc")
# devtools::install_github("collectivemedia/tictoc")
# install.packages("kableExtra")
# install.packages("doParallel", dependencies = TRUE)
# install.packages("randomForest", dependencies = TRUE)
# install.packages("rf")
library(readr)
library(tidyverse)
library(ISLR)
library(caret)
library(orientlib)
library(matrixStats)
library(randomForest)
library(RSpincalc)
library(tictoc)
library(kableExtra)

library(doParallel)
library(foreach)

library(nnet)

library(mlbench)

```

## Load Data

```{r include=FALSE}

# cl <- makePSOCKcluster(10)
# registerDoParallel(cl)
```



```{r include=FALSE}

x_train <- read_csv("data/X_train.csv")
y_train <- read_csv("data/y_train.csv")
x_test <- read_csv("data/x_test.csv")


# join the labels with the training data set
x_train <- x_train %>% left_join(y_train, by = "series_id")
x_train <- as.data.frame(x_train)
x_test <- as.data.frame(x_test)

# x_train <- mutate(x_train, surface = as.factor(surface)) %>% 
# 	select(-group_id)
```

```{r}

convert_quaternions_to_euler <- function(a_dataset){
	
	# use Q2EA from RSpincalc to convert quaternions to euler angles
	Q <- a_dataset %>% select(orientation_X, orientation_Y, orientation_Z, orientation_W) %>% as.matrix()
	
	euler_matrix <- Q2EA(Q, EulerOrder='xyz', tol = 10 * .Machine$double.eps, ichk = FALSE, ignoreAllChk = FALSE)

	# add the new columns to the dataset
	a_dataset <- a_dataset %>% mutate(phi = euler_matrix[,1], theta = euler_matrix[,2], psi = euler_matrix[,3])
	
	# remove quaternion columns
	a_dataset <- a_dataset %>% select(-orientation_X, -orientation_Y,  -orientation_Z, -orientation_W)
	
	# return the new dataset
	a_dataset
}

x_train <- convert_quaternions_to_euler(x_train)
x_test <- convert_quaternions_to_euler(x_test)


```



```{r}
# function for prreprocessing 
# n_of_rows defaults to total number ofseries
# is_train indicates that the data is training, thus will do an extra action
pre_process <- function(a_dataframe, n_of_rows = nrow(a_dataframe)/128 ) {
	
	# get data grouped by seeries_id and compute some means 
	processed_data_df <- a_dataframe %>% 
	group_by(series_id) %>% 
	summarize(
		phi_mean_all = mean(phi),
		phi_sd_all = sd(phi),
		phi_mean_to_sd_all = mean(phi)/sd(phi),
		theta_mean_all = mean(theta),
		theta_sd_all = sd(theta),
		theta_mean_to_sd_all = mean(theta)/sd(theta),
		psi_mean_all = mean(psi),
		psi_sd_all = sd(psi),
		psi_mean_to_sd_all = mean(psi)/sd(psi),
		dist_area = (max(linear_acceleration_X) - min(linear_acceleration_X)) * (max(linear_acceleration_Y) - min(linear_acceleration_Y)) + 
			(max(linear_acceleration_X) - min(linear_acceleration_X)) * (max(linear_acceleration_Z) - min(linear_acceleration_Z)) + 
			(max(linear_acceleration_Y) - min(linear_acceleration_Y)) * (max(linear_acceleration_Z) - min(linear_acceleration_Z)),
		omega_area = (max(angular_velocity_X) - min(angular_velocity_X)) * (max(angular_velocity_Y) - min(angular_velocity_Y)) +
			(max(angular_velocity_X) - min(angular_velocity_X)) * (max(angular_velocity_Z) - min(angular_velocity_Z)) +
			(max(angular_velocity_Y) - min(angular_velocity_Y)) * (max(angular_velocity_Z) - min(angular_velocity_Z)),
		euler_area = (max(phi) - min(phi)) * (max(theta) - min(theta)) + 
			(max(phi) - min(phi)) * (max(psi) - min(psi)) + 
			(max(theta) - min(theta)) * (max(psi) - min(psi)),
		dist_mean_x = mean(linear_acceleration_X),
		dist_mean_y = mean(linear_acceleration_Y),
		dist_mean_z = mean(linear_acceleration_Z),
		omega_mean_x = mean(angular_velocity_X),
		omega_mean_y = mean(angular_velocity_Y),
		omega_mean_Z = mean(angular_velocity_Z),
		dist_sd_x = sd(linear_acceleration_X),
		dist_sd_y = sd(linear_acceleration_Y),
		dist_sd_z = sd(linear_acceleration_Z),
		omega_sd_x = sd(angular_velocity_X),
		omega_sd_y = sd(angular_velocity_Y),
		omega_sd_Z = sd(angular_velocity_Z)	
		
		) %>% 
		slice(1:n_of_rows)
	
	# define an empty data frame with summary metrics that we'll use for each set of 128 observations
	metrics <- c("dist_total","dist_max","dist_min","dist_max_to_min","dist_mean","dist_sd","dist_mean_to_sd",
							 "omega_total","omega_max","omega_min","omega_max_to_min","omega_mean","omega_sd","omega_mean_to_sd",
							 "phi_total","phi_max","phi_min","phi_mean","phi_sd","phi_mean_to_sd",
							 "theta_total","theta_max","theta_min","theta_mean","theta_sd","theta_mean_to_sd",
							 "psi_total","psi_max","psi_min","psi_mean","psi_sd","psi_mean_to_sd",
							 "euler_total","euler_max","euler_min","euler_mean","euler_sd","euler_mean_to_sd")
	tmp_df <- data.frame(matrix(ncol = length(metrics), nrow = 0) )
	colnames(tmp_df) <- metrics

	# loop over each series
	# should use apply type of function here, but I use "for" until I master the apply
	for (s_id in processed_data_df$series_id)
	{
		# get current measurement set
		this_chunk_df <- a_dataframe %>% filter(series_id == s_id)
		
		# select only columns we are interested in 
		this_chunk_df <- this_chunk_df %>% 
			select(
			phi, theta, psi,  
			angular_velocity_X, angular_velocity_Y, angular_velocity_Z,
			linear_acceleration_X, linear_acceleration_Y, linear_acceleration_Z)

		dist_v <- sqrt(diff(this_chunk_df$linear_acceleration_X)^2 + diff(this_chunk_df$linear_acceleration_Y)^2 + diff(this_chunk_df$linear_acceleration_Z)^2)
		omega_v <- sqrt(diff(this_chunk_df$angular_velocity_X)^2 + diff(this_chunk_df$angular_velocity_Y)^2 + diff(this_chunk_df$angular_velocity_Z)^2)
		phi_v <- abs(diff(this_chunk_df$phi))
		theta_v <- abs(diff(this_chunk_df$theta))
		psi_v <- abs(diff(this_chunk_df$psi))

		# fill or temp data frame with summary computations for our 128 measurement set
		tmp_df <- bind_rows(tmp_df, data_frame(
			dist_total = sum(dist_v),
			dist_max = max(dist_v),
			dist_min = min(dist_v),
			dist_max_to_min = max(dist_v)/min(dist_v),
			dist_mean = mean(dist_v),
			dist_sd = sd(dist_v),
			dist_mean_to_sd = mean(dist_v)/sd(dist_v),  # reciprocal coef of variation
			
			omega_total = sum(omega_v),
			omega_max = max(omega_v),
			omega_min = min(omega_v),
			omega_max_to_min = max(omega_v)/min(omega_v),
			omega_mean = mean(omega_v),
			omega_sd = sd(omega_v),
			omega_mean_to_sd = mean(omega_v)/sd(omega_v), # reciprocal coef of variation

			phi_total = sum(phi_v),
			phi_max = max(phi_v),
			phi_min = min(phi_v),
			phi_mean = mean(phi_v),
			phi_sd = sd(phi_v),
			phi_mean_to_sd = mean(phi_v)/sd(phi_v),
			
			theta_total = sum(theta_v),
			theta_max = max(theta_v),
			theta_min = min(theta_v),
			theta_mean = mean(theta_v),
			theta_sd = sd(theta_v),
			theta_mean_to_sd = mean(theta_v)/sd(theta_v),
			
			psi_total = sum(psi_v),
			psi_max = max(psi_v),
			psi_min = min(psi_v),
			psi_mean = mean(psi_v),
			psi_sd = sd(psi_v),
			psi_mean_to_sd = mean(psi_v)/sd(psi_v),
			
			euler_total = sum(phi_v + theta_v + psi_v), 
			euler_max = max(phi_v + theta_v + psi_v),
			euler_min = min(phi_v + theta_v + psi_v),
			euler_mean = mean(phi_v + theta_v + psi_v),
			euler_sd = sd(phi_v + theta_v + psi_v),
			euler_mean_to_sd = mean(phi_v + theta_v + psi_v)/sd(phi_v + theta_v + psi_v)
			))
	
	} # end of for over series
	
	# add the summary computations to the data set of series
	processed_data_df <- bind_cols(processed_data_df, tmp_df)

	# more features
	processed_data_df <- processed_data_df %>% mutate(
		f1 = log(dist_mean_to_sd*omega_mean_to_sd),
		f2 = log(dist_total*omega_total),
		f3 = abs(atan(theta_mean_all/psi_mean_all)))		
	
	# return the proceessed data
	processed_data_df
}

```


```{r preprocess both train and test data and save}

# pre-process train and test data sets
tic("process train data")
x_train_processed <- pre_process(x_train)
toc()

tic("process test data")
x_test_processed <- pre_process(x_test)
toc()

# rejoin train data with the labels data set
x_train_processed <- x_train_processed %>% left_join(y_train, by = "series_id")

write_csv(x_train_processed, "data/x_train_processed.csv")
write_csv(x_test_processed, "data/x_test_processed.csv")

rm(x_train, x_test, y_train)
rm(x_train_processed, x_test_processed)
```

## Load Pre-processed Data From Hard-disk

```{r load data and partition, echo=TRUE, message=FALSE, warning=FALSE}

x_train_processed_from_file <- read_csv("data/x_train_processed.csv")
x_test_processed_from_file <- read_csv("data/x_test_processed.csv")

# if we load data from a file, convert surface to factor
x_train_processed_from_file <- x_train_processed_from_file %>% mutate(surface = as.factor(surface))

# use a smaller set of datab to save time
x_train_processed_from_file <- x_train_processed_from_file # %>% slice(1:1000)



```


## Select Features We Want and Partition Data

```{r include=FALSE}

x_test_processed <- x_test_processed_from_file

# remove group_id
x_train_processed <- x_train_processed_from_file # %>% select(-group_id)

# rm(x_test_processed_from_file, x_train_processed_from_file)

s8 <- read_csv("data/submission_08.csv")
s6 <- read_csv("data/submission_06.csv")
s5 <- read_csv("data/submission_05.csv")
s4 <- read_csv("data/submission_04.csv")
s3 <- read_csv("data/submission_03.csv")
s2 <- read_csv("data/submission_RF_untuned0_02.csv")
s10 <- read_csv("data/model_10_fit_submission.csv")
s11 <- read_csv("data/model_11_fit_submission.csv")

x_pool_sub <- s6 %>%
	inner_join(s11, by = c("series_id", "surface")) %>% 
	inner_join(s10, by = c("series_id", "surface")) %>% 
	inner_join(s8, by = c("series_id", "surface")) %>% 
	inner_join(s6, by = c("series_id", "surface")) %>% 
	inner_join(s5, by = c("series_id", "surface")) %>% 
	inner_join(s4, by = c("series_id", "surface")) %>% 
	inner_join(s3, by = c("series_id", "surface")) %>% 
	inner_join(s2, by = c("series_id", "surface")) 


x_pool_sub <- x_test_processed %>% inner_join(x_pool_sub, by = "series_id") %>% 
	mutate(series_id = series_id * 1000000)

x_pool_sub <- x_pool_sub %>% mutate(surface = as.factor(surface))

# x_train_processed <- bind_rows(x_train_processed, x_pool_sub)




# convert both test and train data to matrix in order to analyse featuree corelation
x_train_matrix <- x_train_processed %>% select(-surface, -series_id) %>% as.matrix()
x_test_matrix <- x_test_processed %>% select(-series_id) %>% as.matrix()

# find features that are high correlated 
# find linear dependencies and eliminate them
# names_to_removed_train <- findCorrelation(cor(x_train_matrix), cutoff = 0.9, names = TRUE, verbose = FALSE, exact=TRUE)
names_to_remove_test <- findCorrelation(cor(x_test_matrix), cutoff = 0.95, names = TRUE, verbose = FALSE, exact=TRUE)

# remove correlated features from both train and test sets
x_train_processed <- x_train_processed %>% select(-names_to_remove_test) 
x_test_processed <- x_test_processed %>% select(-names_to_remove_test) 

rm(x_train_matrix, x_test_matrix, names_to_removed_train, names_to_remove_test)
rm(s2, s3, s4, s5, s6, s8, s10, s11, x_pool_sub)

# ########################################################################################################
# pre-process the data, center and scale the values across all predictors
pre_process <- x_train_processed %>% select(-series_id, -group_id) %>% preProcess(method = c("center", "scale"))
x_train_processed <- predict(pre_process, x_train_processed)
x_test_processed <- predict(pre_process, x_test_processed)

# ########################################################################################################
# remove columns that aree not important in classification
x_train_processed <- x_train_processed %>% select(-phi_mean_to_sd, -dist_mean_x, -dist_mean_y, -dist_mean_z, -omega_max_to_min )
x_test_processed <- x_test_processed %>% select(-phi_mean_to_sd, -dist_mean_x, -dist_mean_y, -dist_mean_z, -omega_max_to_min )


# ########################################################################################################
# create Mahalanobis distance to each of the class centroids
# see: https://topepo.github.io/caret/pre-processing.html#the-preprocess-function
# tmp <- x_train_processed %>%
# 	select(-series_id, -surface, -group_id) %>%
# 	as.matrix()
# 
# centroids <-	classDist(tmp, x_train_processed$surface, pca = TRUE, keep = 20)
# 
# x_train_processed <- bind_cols(x_train_processed,as.data.frame(predict(centroids, x_train_processed)))
# x_test_processed <- bind_cols(x_test_processed, as.data.frame(predict(centroids, x_test_processed)))
# 
# 
# x_train_processed <- bind_cols(select(x_train_processed, series_id, group_id, surface, f3, f2),
# 															as.data.frame(predict(centroids, x_train_processed)))
# x_test_processed <- bind_cols(select(x_test_processed, series_id, f3, f2),
# 															as.data.frame(predict(centroids, x_test_processed)))

# ########################################################################################################
# partition x_train_processed data for training and testing 
test_index <- createDataPartition(y = x_train_processed$surface, times = 1, p = 0.5, list = FALSE)
x_train_for_train <- x_train_processed[-test_index, ]
x_train_for_test <- x_train_processed[test_index, ]

#x_train_for_train_t <- minDiss()

# # draw an image of distances between each point to all others
# x <- x_train_processed %>% arrange(surface) %>%  select(-series_id, -group_id, -surface) %>%  as.matrix()
# d <- dist(x)
# image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, "RdBu")))
```



```{r}
models <- c("naive_bayes",  "svmLinear", 
            "gamboost",  "gamLoess", "qda", 
            "knn", "kknn", "loclda", "gam",
            "rf", "ranger",  "wsrf", "Rborist", 
            "avNNet", "mlp", "monmlp",
            "adaboost", "gbm",
            "svmRadial", "svmRadialCost", "svmRadialSigma")

models <- c(
	"avNNet",	# Model Averaged Neural Network
	"monmlp"	# Monotone Multi-Layer Perceptron Neural Network
	)

x <- x_train_for_train %>% select(-series_id) # %>% slice(1:500)

# train data for each model and store in model_fits
model_fits <- lapply(models, function(model){
  print(model)
  train(surface ~., method = model, data = x)
  
})

# set a column for each model
names(model_fits) <- models

# predict for each model generated and store results in model_predictions
model_predictions <- sapply(model_fits, function(model){
  
	y_hat <- predict(model, select(x_train_for_test, -series_id))
	conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
	conf_matrix$overall["Accuracy"]
  
})

control <- trainControl(size = 5, bag = TRUE)
model_fit  <- train(surface ~., method = "avNNet", data = x, trControl=control)
y_hat <- predict(model_fit, select(x_train_for_test, -series_id))
conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
conf_matrix$overall["Accuracy"]

model_fit  <- train(surface ~., method = "monmlp", data = x)
y_hat <- predict(model_fit, select(x_train_for_test, -series_id))
conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
conf_matrix$overall["Accuracy"]

##### 
# gmb
x <- x_train_for_train %>% select(-series_id, -group_id) %>% slice(1:500)

fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
nrow(gbmGrid)
model_fit <- train(surface ~ ., data = x, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 ## Now specify the exact models 
                 ## to evaluate:
                 tuneGrid = gbmGrid)

y_hat <- predict(model_fit, select(x_train_for_test, -series_id))
conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
conf_matrix$overall["Accuracy"]

###

model_fit  <- train(surface ~., method = "C5.0", data = x)
y_hat <- predict(model_fit, select(x_train_for_test, -series_id))
conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
conf_matrix$overall["Accuracy"]

# predict
y_hat <- predict(model_fit, select(x_test_processed, -series_id))

x_test_processed_for_submission <- x_test_processed %>% select(series_id) %>% mutate(surface = y_hat)
write_csv(x_test_processed_for_submission, "data/submission_C50_02.csv")

```


## KNN


```{r}

tic(" fit model knn")
fit <- train(surface ~ f3 + f2 + phi_mean_all + theta_mean_all + psi_mean_all,  method = "knn", 
             tuneGrid = data.frame(k = seq(2, 100, 2)), 
             data = select(x_train_for_train, -series_id, -group_id))
toc()

ggplot(fit) 

# fit <- train(surface ~ ., method = "knn", data = x_train_for_test, k = 36)

fit$bestTune

y_hat <- predict(fit, x_train_for_test, type = "raw")
conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
conf_matrix$overall["Accuracy"]
conf_matrix$table


```

## RPart

```{r}
model_fit <- train(surface ~ ., 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.075, len = 40)),
                     data = select(x_train_for_train, -series_id, -group_id))
ggplot(model_fit)

confusionMatrix(predict(model_fit, x_train_for_test), x_train_for_test$surface)$overall["Accuracy"]
```

## Random Forest Untuned

```{r, fig.height=9, fig.width = 16}

model_fit <- randomForest(
	surface ~ ., 
	metric = "Accuracy", 
	data = slice(select(x_train_for_train, -series_id, -group_id), 1:5000)
	)

y_hat <- predict(model_fit, select(x_train_for_test, -series_id))
y_test <- x_train_for_test$surface

conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
conf_matrix$overall["Accuracy"]
conf_matrix$table %>% knitr::kable()

importance <- importance(model_fit)
# importance[order(importance[,1], decreasing = TRUE), ]
varImpPlot(model_fit)

# model_fit_RF_untuned <- randomForest(surface ~ ., data = x_train_processed)
# 
# # predict
# y_hat <- predict(model_fit_RF_untuned, select(x_test_processed, -series_id))
# 
# x_test_processed_for_submission <- x_test_processed %>% select(series_id) %>% mutate(surface = y_hat)
# write_csv(x_test_processed_for_submission, "data/submission_RF_untuned0_02.csv")


```


## Random Forests Customized one-vs-one

```{r randomTrees, fig.height=9, fig.width = 16}

# control <- trainControl(method="repeatedcv", number=2, repeats=10, search="grid", sampling = "up")
control <- trainControl(method="repeatedcv", number=2, repeats=20, search="grid")

metric <- "Accuracy"

mtry <- sqrt(ncol(x_train_for_train) - 1)
mtry <- 4:8
tunegrid <- expand.grid(.mtry=mtry,.ntree=c(100, 200, 500, 1000, 1500))

# tunegrid <- expand.grid(.mtry=c(1:10))

customRF 						<- 	list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- 	data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid 			<- 	function(x, y, len = NULL, search = "grid") {}
customRF$fit 				<- 	function(x, y, wts, param, lev, last, weights, classProbs, ...) randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
customRF$predict 		<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL) predict(modelFit, newdata)
customRF$prob 			<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL)	predict(modelFit, newdata, type = "prob")
customRF$sort 			<- 	function(x) x[order(x[,1]),]
customRF$levels 		<- 	function(x) x$surface


model_fit_rf <- train(surface ~ ., 
											data = slice(select(x_train_for_train, -series_id, -group_id), 1:500), 
											#data = x_train_for_train, 
											method=customRF, 
											metric=metric, 
											tuneGrid=tunegrid, 
											trControl=control
											)
print(model_fit_rf)
plot(model_fit_rf)

y_hat <- predict(model_fit_rf, select(x_train_for_test, -series_id))
y_test <- x_train_for_test$surface

importance <- importance(model_fit_rf$finalModel)
importance[order(importance[,1], decreasing = TRUE), ] 
varImpPlot(model_fit_rf$finalModel)

conf_matrix <- confusionMatrix(y_hat, x_train_for_test$surface)
conf_matrix$overall["Accuracy"]
conf_matrix$table

model_fit_rf$finalModel$importance %>% order[MeanDecreaseGini, ]



# cl <- makePSOCKcluster(10)
# registerDoParallel(cl)
# control <- trainControl(method="repeatedcv", number=20, repeats=2, search="grid", classProbs = TRUE, sampling = "up")
# 
# model_fit_rf <- train(surface ~ ., 
# 											data = slice(select(x_train_processed, -series_id), 1:5000), 
# 											#data = x_train_for_train, 
# 											method=customRF, 
# 											metric=metric, 
# 											tuneGrid=tunegrid, 
# 											trControl=control
# 											)
# 
# # stopCluster(cl) 
# 
# # predict
# y_hat <- predict(model_fit_rf$finalModel, select(x_test_processed, -series_id))
# 
# x_test_processed_for_submission <- x_test_processed %>% select(series_id) %>% mutate(surface = y_hat)
# write_csv(x_test_processed_for_submission, "data/submission_customRF_one_vs_all_03.csv")
```


## Produce Data For Submission

```{r}
# get just the series_id to be merged later with y_hat
test_series_id <- x_test_processed %>% select(series_id)

# remove series_id from the data set in order to make the predictions
x_test_processed_for_predict <- x_test_processed %>% select(-series_id)

# read the saved model
model_fit_RF <- readRDS("models/model_fit_RF_one_vs_one.rds")

# predict
y_hat <- predict(model_fit_RF, x_test_processed_for_predict)

# produce Kaggle data for submission

submission <- test_series_id %>% mutate(surface = y_hat)
write_csv(submission, "data/submission_02.csv")

```


## Create Models for One vs All Scenario

Here we will perform a binary classification for each of the class.
We will keep current surface and rename the rest of the surfaces to "the_rest".
Qw will save each modeel on a file on hard-disk

```{r one-vs-all}
# get a small chunk for now
x_train_processed_ova <- x_train_processed   #%>% slice(1:1500)

# a prefix to save models on hdd
model_prefix <- "model_13_fit_"

# # partition data into train and test
# test_index <- createDataPartition(y = x_train_processed_ova$surface, times = 1, p = 0.5, list = FALSE)
# x_train_for_train_ova <- x_train_processed_ova[-test_index, ]
# x_train_for_test_ova <- x_train_processed_ova[test_index, ]

# partition data into:train, test, and balancing pool
# we will use the pool to extract records to balance the dataset
folds <- createFolds(x_train_processed_ova$surface, k = 3, list = TRUE)
x_train_for_train_ova <- x_train_processed_ova[folds$Fold1,]
x_train_for_test_ova <- x_train_processed_ova[folds$Fold2,]
x_train_pool <- x_train_processed_ova[folds$Fold3,]

# instead of balancing, we merge the pool with the train set
# x_train_for_train_ova <- bind_rows(x_train_for_train_ova, x_train_pool)


# get surfaces in a data frame, so we can loop over
surfaces <- x_train_for_train_ova %>% group_by(surface) %>% 
	summarize(n = n()) %>% 
	mutate(surface = as.character(surface)) %>% 
	# filter(surface == "hard_tiles") %>% 
	arrange(n)

for(current_surface in surfaces$surface)
{
		# convert surface to two values
		# x_train_for_train_ova_current <- x_train_for_train_ova %>% 
		# 	mutate(surface = ifelse(surface == current_surface, current_surface, "the_rest")) %>% 
		# 	mutate(surface = as.factor(surface))	
		
		# do training of full train data
		x_train_for_train_ova_current <- x_train_for_train_ova %>% 
			mutate(surface = ifelse(surface == current_surface, current_surface, "the_rest")) %>% 
			mutate(surface = as.factor(surface))
		
		# add records from thee pool to balance the recordset
		x_chunk_for_balance <- x_train_pool %>% filter(surface == current_surface)
		x_train_for_train_ova_current <- bind_rows(x_train_for_train_ova_current, x_chunk_for_balance)
		
		###################################################################################################
		# custom randomForest
		# 
		mtry <- sqrt(ncol(x_train_for_train_ova_current) - 1)
		tunegrid <- expand.grid(.mtry=mtry,.ntree=c(100, 300, 400, 500, 1000, 1500, 2000))
		control <- trainControl(method="repeatedcv", 
														number=20, 
														repeats=2, 
														search="grid", 
														classProbs = TRUE, 
														summaryFunction = twoClassSummary,
														sampling = "up")
		customRF 						<- 	list(type = "Classification", library = "randomForest", loop = NULL)
		customRF$parameters <- 	data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
		customRF$grid 			<- 	function(x, y, len = NULL, search = "grid") {}
		customRF$fit 				<- 	function(x, y, wts, param, lev, last, weights, classProbs, ...) randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
		customRF$predict 		<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL) predict(modelFit, newdata)
		customRF$prob 			<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL)	predict(modelFit, newdata, type = "prob")
		customRF$sort 			<- 	function(x) x[order(x[,1]),]
		customRF$levels 		<- 	function(x) x$surface
				
		model_fit_current <- train(surface ~ ., 
															 data = select(x_train_for_train_ova_current, -series_id), 
															 method=customRF, 
															 metric="ROC", 
															 tuneGrid=tunegrid, 
															 trControl=control)
		###################################################################################################
		
		###################################################################################################
		# model_fit_current <- train(surface ~ ., data = x_train_for_train_ova_current,
		# 		method = "rf",
		# 		metric = "ROC",
		# 		tuneGrid = data.frame(mtry = mtry),
		# 		trControl = trainControl(method = "cv",
		# 		                        classProbs = TRUE,
		# 		                        summaryFunction = twoClassSummary))
		# 
		###################################################################################################

		
		# print(model_fit_current)
		# plot(model_fit_current)
		
		model_name <- paste(model_prefix, current_surface, sep = "")
		file <- paste("models/",  model_name, ".rds", sep = "")
		write_rds(model_fit_current, file)
}


```


## Load Models and Perform the Prediction

Loop over each surface, load the model from hard-disk and perform the prediction.


```{r}


# get surfaces in a data frame, so we can loop over
surfaces <- x_train_for_train_ova %>% group_by(surface) %>% 
	summarize(n = n()) %>% 
	mutate(surface = as.character(surface)) %>% 
	arrange(n)

# store final result in this table
results_df <- data.frame(surface = "Surface", model_name = "Model Name", ova_accuracy = "One-vs-All Accuracy")

# create a data frame that will store the results from each model
# set it up with series_id and true_surface initially, i'll usee this one mostly for pre-viewing the data
results <- data.frame(
	series_id = x_train_for_test_ova$series_id, 
	true_surface = x_train_for_test_ova$surface)

# results <- results %>% mutate(rowNum = row_number()) 

# create a data frame the will store probabilities for each model
# we'll use this for voting
# the model with highes prediction will get the vote

results_voting <- data.frame(
	series_id = x_train_for_test_ova$series_id, 
	true_surface = x_train_for_test_ova$surface)

# filter for a surface of interest
# surfaces <- surfaces %>% filter(surface == "hard_tiles")
# surfaces <- surfaces %>% filter(surface == "concrete")

foreach(current_surface = surfaces$surface) %dopar% {
	
	# prepare the test dataset: we keep current surfacee name, and we rename all other surfaces to "the_rest"
	# we have now a binary clasification.
	x_train_for_test_ova_current <- x_train_for_test_ova %>% 
			mutate(surface = ifelse(surface == current_surface, current_surface, "the_rest")) %>% 
			mutate(surface = as.factor(surface))
	
	# get the modeel from a file
	model_name <- paste(model_prefix, current_surface, sep = "")
	model_fit_current <- readRDS(paste("models/", model_name, ".rds", sep = ""))
	
	# get y_hat and y_hat_prob
	y_hat <- predict(
										model_fit_current, 
									 	select(x_train_for_test_ova_current, -series_id), 
										type = "raw")
	y_hat_prob <- predict(
										model_fit_current, 
										select(x_train_for_test_ova_current, -series_id), 
										type = "prob")
	
	# get the y_test
	y_test <-  x_train_for_test_ova_current$surface
	
	# store results
	results <- results %>% mutate(last_result = y_hat)
	names(results)[ncol(results)] <- paste(current_surface, "_pred", sep = "")
	
	# store the results as probability of the minority class. that is our surface
	results <- results %>% mutate(last_result_prob = y_hat_prob[,current_surface])
	names(results)[ncol(results)] <- paste(current_surface, "_prob", sep = "")
	
	# store the probability of curent model for current surface in a column named by current surface
	results_voting <- results_voting %>% mutate(last_result_prob = y_hat_prob[,current_surface])
	names(results_voting)[ncol(results_voting)] <- current_surface # the column name is current surface
	
		# store accuracy
	results_df <- results_df %>% bind_rows(data.frame(surface = current_surface, model_name = model_name, ova_accuracy = as.character(conf_matrix$overall["Accuracy"] )))
	
}

# add an empty column for predicted surfaces 
results_voting <- results_voting %>%  mutate(pred_surface = rep("", nrow(results_voting)))

# set the value on predicted surface to the surface that got maximum probability
for (i in 1:nrow(results_voting)) {
		results_voting[i, "pred_surface"] <- names(which.max(select(results_voting[i,], -series_id, -true_surface, -pred_surface)))
}

results_voting <- results_voting %>% mutate(pred_surface = as.factor(pred_surface))

# overall accuracy
# sum(results_voting[, "true_surface"] == results_voting[, "pred_surface"])/nrow(results_voting)

# compute confusion matrix and print it
conf_matrix <- confusionMatrix(results_voting$pred_surface,
															 results_voting$true_surface)
print(conf_matrix$overall["Accuracy"])
print(conf_matrix$table)


```


## Evaluate Test Dataset and Submit

```{r}

# get surfaces in a data frame, so we can loop over
surfaces <- x_train_for_train_ova %>% group_by(surface) %>% 
	summarize(n = n()) %>% 
	mutate(surface = as.character(surface)) %>% 
	arrange(n)

# create a data frame the will store probabilities for each model
# we'll use this for voting
# the model with highes prediction will get the vote
results_voting <- data.frame(
	series_id = x_test_processed$series_id)


for(current_surface in surfaces$surface){
	
	# get the model from a file
	model_name <- paste(model_prefix, current_surface, sep = "")
	model_fit_current <- readRDS(paste("models/", model_name, ".rds", sep = ""))
	
	# get y_hat and y_hat_prob
	y_hat <- predict(
							model_fit_current, 
							select(x_test_processed, -series_id), 
							type = "raw")
	y_hat_prob <- predict(
							model_fit_current, 
							select(x_test_processed, -series_id), 
							type = "prob")

	# store the probability of curent model for current surface in a column named by current surface
	results_voting <- results_voting %>% mutate(last_result_prob = y_hat_prob[,current_surface])
	names(results_voting)[ncol(results_voting)] <- current_surface # the column name is current surface
	
}

# results_voting
results_voting <- results_voting %>%  mutate(surface = rep("", nrow(results_voting)))

for (i in 1:nrow(results_voting)) {
		results_voting[i, "surface"] <- names(which.max(select(results_voting[i,], -series_id, -surface)))
}

results_voting <- results_voting %>% mutate(surface = as.factor(surface))

# create the submission data
submission <- results_voting %>% select(series_id, surface)

# save the file on the file system
file_name <- paste("data/",model_prefix, "submission.csv", sep = "")
write_csv(submission, file_name)


```

```{r}
# stopCluster(cl)
```


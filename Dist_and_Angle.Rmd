---
title: "Surface Detection by Robot Movements"
author: "Marian Dumitrascu"
date: "March 19, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Surface Detection by Robot Movements


## Introduction

gyros, accelerometer and magnetometer sensor

## Data Analysis

You can include R code in the document as follows:

```{r data load, message=FALSE, warning=FALSE}
# install.packages("ISLR")
# install.packages("orientlib")
# install.packages("RSpincalc")
# devtools::install_github("collectivemedia/tictoc")
library(readr)
library(tidyverse)
library(ISLR)
library(caret)
library(orientlib)
library(matrixStats)
library(randomForest)
library(RSpincalc)
library(tictoc)


x_train <- read_csv("data/X_train.csv")
y_train <- read_csv("data/y_train.csv")
x_test <- read_csv("data/x_test.csv")
```
```{r}

# nrow(x_test)/128

```


```{r data analysis}

# join the labels with the training data set
train_set <- x_train %>% left_join(y_train, by = "series_id")
train_df <- as.data.frame(train_set)
test_df <- as.data.frame(x_test)

train_df <- mutate(train_df, surface = as.factor(surface)) %>% 
	select(-group_id)

```

```{r}

convert_quaternions_to_euler <- function(a_dataset){
	# use Q2EA from RSpincalc to convert quaternions to euler angles
	Q <- a_dataset %>% select(orientation_X, orientation_Y, orientation_Z, orientation_W) %>% as.matrix()
	euler_matrix <- Q2EA(Q, EulerOrder='xyz', tol = 10 * .Machine$double.eps, ichk = FALSE, ignoreAllChk = FALSE)
	
	# # same thing can be acheved by this, but I preffer using RSpincalc
	# a <- train_df$orientation_X
	# b <- train_df$orientation_Y
	# c <- train_df$orientation_Z
	# d <- train_df$orientation_W
	# 	
	# phi_v <- atan(2 * (a * b + c * d)/(a^2 - b^2 - c^2 + d^2))
	# theta_v <- -asin(2 * (b * d - a * c))
	# psi_v <- atan(2 * (a * d + b * c)/(a^2 + b^2 - c^2 - d^2))
	
	# add the new columns to the dataset
	a_dataset <- a_dataset %>% mutate(phi = euler_matrix[,1], theta = euler_matrix[,2], psi = euler_matrix[,3])
	
	# remove quaternion columns
	a_dataset <- a_dataset %>% select(-orientation_X, -orientation_Y,  -orientation_Z, -orientation_W)
	
	# return the new dataset
	a_dataset
}

train_df <- convert_quaternions_to_euler(train_df)
test_df <- convert_quaternions_to_euler(test_df)

# train_df[is.na(train_df)]

head(train_df, 200) %>% knitr::kable()
mean(train_df$angular_velocity_X)/sd(train_df$angular_velocity_X)
mean(train_df$angular_velocity_Y)/sd(train_df$angular_velocity_Y)
mean(train_df$angular_velocity_Z)/sd(train_df$angular_velocity_Z)

mean(train_df$linear_acceleration_X)/sd(train_df$linear_acceleration_X)
mean(train_df$linear_acceleration_Y)/sd(train_df$linear_acceleration_Y)
mean(train_df$linear_acceleration_Z)/sd(train_df$linear_acceleration_Z)


train_df %>% select(phi, theta, psi) %>% head(200)
```

```{r histograms, eval=FALSE, include=FALSE}

qplot(train_df$phi, bins =  200)
qplot(train_df$theta, bins =  200)
qplot(train_df$psi, bins =  200)

temp <- train_df %>% filter(series_id == 13)

qplot(temp$phi, bins =  10)
qplot(temp$theta, bins =  20)
qplot(temp$psi, bins =  20)

# temp0 <- train_df %>% group_by(series_id, surface) %>% summarize( n=n(),
# 	mean_phi = sd(phi)/mean(phi),
# 	mean_theta = sd(theta)/mean(theta),
# 	mean_psi = sd(psi)/mean(psi)
# 	)

# temp0 <- train_df %>% group_by(series_id, surface) %>% summarize( n=n(),
# 	mean_phi =mean(phi)/(sd(phi)),
# 	mean_theta = mean(theta)/sd(theta),
# 	mean_psi = mean(psi)/sd(psi)
# 	)
# 	

# temp0 <- train_df %>% group_by(series_id, surface) %>% summarize( n=n(),
# 	mean_phi = mean(phi)/sd(phi),
# 	mean_theta = mean(theta)/sd(theta),
# 	mean_psi = mean(psi)/sd(psi)
# 	)

temp0 <- train_df %>% group_by(series_id, surface) %>% summarize( n=n(),
	mean_phi = mean(phi),
	mean_theta = mean(theta),
	mean_psi = mean(psi)
	)
temp <- temp0 %>% filter(surface == "carpet") 
qplot(temp$mean_phi, bins = 50)
qplot(temp$mean_theta, bins = 50)
qplot(temp$mean_psi, bins = 50)
#qplot(temp$mean_d, bins = 100)

```



```{r group data on training set}


# train_df %>% group_by(group_id, surface) %>% filter(surface == "carpet") %>% 
# 	summarize(measuremeent = n_distinct(series_id)) %>% arrange(group_id)

train_df %>% group_by(surface) %>% 
	summarize(measuremeent = n_distinct(series_id)) %>% 
	arrange(measuremeent)

```




## Compute total distance and total rotation

```{r}
# 3010 observations
# 
measurement_numbers <- train_df %>% slice(1:128) %>% pull(measurement_number)
mn_02 <- as.character(1000 + measurement_numbers)
# class(measurement_numbers)


t_01 <- data.frame(measurement_numbers = NULL,  r = NULL, series_id = NULL)
```



```{r}
# function for prreprocessing 
# n_of_rows defaults to total number ofseries
# is_train indicates that the data is training, thus will do an extra action
pre_process <- function(a_dataframe, n_of_rows = nrow(a_dataframe)/128 ) {
	
	# get data grouped by seeries_id and compute some means 
	processed_data_df <- a_dataframe %>% 
	group_by(series_id) %>% 
	summarize(
		phi_mean_all = mean(phi),
		phi_sd_all = sd(phi),
		theta_mean_all = mean(theta),
		theta_sd_all = sd(theta),
		psi_mean_all = mean(psi),
		psi_sd_all = sd(psi)
		) %>% 
		slice(1:n_of_rows)
	
	# define an empty data frame with summary metrics that we'll use for each set of 128 observations
	metrics <- c("dist_total","dist_max","dist_min","dist_max_to_min","dist_mean","dist_sd","dist_mean_to_sd",
							 "omega_total","omega_max","omega_min","omega_max_to_min","omega_mean","omega_sd","omega_mean_to_sd",
							 "phi_total","phi_max","phi_min","phi_mean","phi_sd","phi_mean_to_sd",
							 "theta_total","theta_max","theta_min","theta_mean","theta_sd","theta_mean_to_sd",
							 "psi_total","psi_max","psi_min","psi_mean","psi_sd","psi_mean_to_sd",
							 "euler_total","euler_max","euler_min","euler_mean","euler_sd","euler_mean_to_sd")
	tmp_df <- data.frame(matrix(ncol = length(metrics), nrow = 0) )
	colnames(tmp_df) <- metrics

	# loop over each series
	# should use apply type of function here, but I use "for" until I master the apply
	for (s_id in processed_data_df$series_id)
	{
		# get current measurement set
		this_chunk_df <- a_dataframe %>% filter(series_id == s_id)
		
		# select only columns we are interested in 
		this_chunk_df <- this_chunk_df %>% 
			select(
			phi, theta, psi,  
			angular_velocity_X, angular_velocity_Y, angular_velocity_Z,
			linear_acceleration_X, linear_acceleration_Y, linear_acceleration_Z)

		# # create some 0 filled vectors for distance, movement angles and euler orientation angles
		# # we will fill them in the following loop
		# dist_v <- rep(0, 127)
		# omega_v <- rep(0, 127)
		# phi_v <- rep(0, 127)
		# theta_v <- rep(0, 127)
		# psi_v <- rep(0, 127)
		# 
		# # loop over each measurement but skip the first one
		# for (i in 2:128)
		# {
		# 	x1 <- this_chunk_df[i-1, ]
		# 	x2 <- this_chunk_df[i, ]
		# 	
		# 	# calculate current distance segment from accelerometer
		# 	this_segment_dist <-	sqrt(
		# 				(x2$linear_acceleration_X - x1$linear_acceleration_X)^2 +
		# 				(x2$linear_acceleration_Y - x1$linear_acceleration_Y)^2 +
		# 				(x2$linear_acceleration_Z - x1$linear_acceleration_Z)^2
		# 		)
		# 
		# 	# calculate current angle velocity change from magnetometer
		# 	this_segment_omega <- sqrt(
		# 				(x2$angular_velocity_X - x1$angular_velocity_X)^2 +
		# 				(x2$angular_velocity_Y - x1$angular_velocity_Y)^2 +
		# 				(x2$angular_velocity_Z - x1$angular_velocity_Z)^2
		# 		)
		# 	
		# 	dist_v[i - 1] <- this_segment_dist
		# 	omega_v[i - 1] <- this_segment_omega
		# 	phi_v[i - 1] <- abs(x2$phi - x1$phi)
		# 	theta_v[i - 1] <- abs(x2$theta - x1$theta)
		# 	psi_v[i - 1] <- abs(x2$psi - x1$psi)
		# 
		# } # end of loop over lines

		dist_v <- sqrt(diff(this_chunk_df$linear_acceleration_X)^2 + diff(this_chunk_df$linear_acceleration_Y)^2 + diff(this_chunk_df$linear_acceleration_Z)^2)
		omega_v <- sqrt(diff(this_chunk_df$angular_velocity_X)^2 + diff(this_chunk_df$angular_velocity_Y)^2 + diff(this_chunk_df$angular_velocity_Z)^2)
		phi_v <- abs(diff(this_chunk_df$phi))
		theta_v <- abs(diff(this_chunk_df$theta))
		psi_v <- abs(diff(this_chunk_df$psi))

		# fill or temp data frame with summary computations for our 128 measurement set
		tmp_df <- bind_rows(tmp_df, data_frame(
			dist_total = sum(dist_v),
			dist_max = max(dist_v),
			dist_min = min(dist_v),
			dist_max_to_min = max(dist_v)/min(dist_v),
			dist_mean = mean(dist_v),
			dist_sd = sd(dist_v),
			dist_mean_to_sd = mean(dist_v)/sd(dist_v),  # reciprocal coef of variation
			
			omega_total = sum(omega_v),
			omega_max = max(omega_v),
			omega_min = min(omega_v),
			omega_max_to_min = max(omega_v)/min(omega_v),
			omega_mean = mean(omega_v),
			omega_sd = sd(omega_v),
			omega_mean_to_sd = mean(omega_v)/sd(omega_v), # reciprocal coef of variation

			phi_total = sum(phi_v),
			phi_max = max(phi_v),
			phi_min = min(phi_v),
			phi_mean = mean(phi_v),
			phi_sd = sd(phi_v),
			phi_mean_to_sd = mean(phi_v)/sd(phi_v),
			
			theta_total = sum(theta_v),
			theta_max = max(theta_v),
			theta_min = min(theta_v),
			theta_mean = mean(theta_v),
			theta_sd = sd(theta_v),
			theta_mean_to_sd = mean(theta_v)/sd(theta_v),
			
			psi_total = sum(psi_v),
			psi_max = max(psi_v),
			psi_min = min(psi_v),
			psi_mean = mean(psi_v),
			psi_sd = sd(psi_v),
			psi_mean_to_sd = mean(psi_v)/sd(psi_v),
			
			euler_total = sum(phi_v + theta_v + psi_v), 
			euler_max = max(phi_v + theta_v + psi_v),
			euler_min = min(phi_v + theta_v + psi_v),
			euler_mean = mean(phi_v + theta_v + psi_v),
			euler_sd = sd(phi_v + theta_v + psi_v),
			euler_mean_to_sd = mean(phi_v + theta_v + psi_v)/sd(phi_v + theta_v + psi_v)
			))
	
	} # end of for over series
	
	# add the summary computations to the data set of series
	processed_data_df <- bind_cols(processed_data_df, tmp_df)

	##########################
	# use PCA, maybe later again
	# pca <- prcomp(select(processed_data_df, -surface))
	# summary(pca)
	# processed_data_df <- processed_data_df %>% mutate(PC1 = pca$x[,1], PC2 = pca$x[,2]) %>% select(-dist, -omega, -phi, -theta, -psi, -mean_sd_dist)
	#########################
	
	# return the proceessed data
	processed_data_df
}

```


```{r preprocess both train and test data}

# pre-process train and test data sets
tic("process train data")
x_train_processed <- pre_process(train_df, 1000)
toc()

tic("process test data")
x_test_processed <- pre_process(test_df)
toc()

write_csv(x_train_processed, "data/x_train_processed_w_diff.csv")

# rejoin train data with the labels data set
x_train_processed <- x_train_processed %>% left_join(y_train, by = "series_id")

# remove series_id
x_train_processed <- x_train_processed %>% select(-series_id)


# split data for training
test_index <- createDataPartition(y = x_train_processed$surface, times = 1, p = 0.5, list = FALSE)
train_train_partition <- x_train_processed[-test_index, ]
train_test_partition <- x_train_processed[test_index, ]

end_time <- Sys.time()
end_time - start_time
```





```{r, fig.width=24, fig.height=16, fig.retina=2}

x <- train_train_partition %>% select(-surface) %>% as.matrix()
y <- train_train_partition$surface

x_train_processed %>%  ggplot(aes(dist_total, mean_sd_dist, fill = surface)) +
	geom_point(aes(color = surface))

x_train_processed %>%  ggplot(aes(mean_phi, mean_theta, fill = surface)) +
	geom_point(aes(color = surface))

x_train_processed %>%  ggplot(aes(mean_phi, mean_psi, fill = surface)) +
	geom_point(aes(color = surface))

x_train_processed %>%  ggplot(aes(mean_theta, mean_psi, fill = surface)) +
	geom_point(aes(color = surface))

x_train_processed %>%  ggplot(aes(omega_total, mean_sd_omega, fill = surface)) +
	geom_point(aes(color = surface))

x_train_processed %>%  ggplot(aes(dist_total, omega_total, fill = surface)) +
	geom_point(aes(color = surface))

x_train_processed %>%  ggplot(aes(dist_total, phi_total + theta_total + psi_total, fill = surface)) +
	geom_point(aes(color = surface))

# x_train_processed %>% filter(surface == "soft_tiles")
```



```{r}




# t_04 %>% ggplot(aes(PC1, PC2, fill = surface)) +
# 	geom_point(aes(color = surface)) +
# 	geom_point(cex=3, pch=21) +
#   coord_fixed(ratio = 1)
```

```{r}

# install.packages("ISLR")
fit <- train(surface ~ . ,  method = "knn", 
             tuneGrid = data.frame(k = seq(2, 100, 2)), 
             data = train_train_partition)
ggplot(fit) 

# fit <- train(surface ~ ., method = "knn", data = train_test_partition, k = 36)

fit$bestTune

# y_hat <- predict(fit$finalModel, train_test_partition, type = "class")
y_hat <- predict(fit, train_test_partition, type = "raw")
conf_matrix <- confusionMatrix(y_hat, train_test_partition$surface)
conf_matrix$overall["Accuracy"]
conf_matrix$table
```

```{r}
train_rpart <- train(surface ~ ., 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.075, len = 40)),
                     data = train_train_partition)
ggplot(train_rpart)

confusionMatrix(predict(train_rpart, train_test_partition), train_test_partition$surface)$overall["Accuracy"]
```


```{r}

fit_model_RF_untuned <- randomForest(surface ~ ., data = train_train_partition) 
y_hat <- predict(fit_model_RF_untuned, train_test_partition)
y_test <- train_test_partition$surface

sum(y_hat == y_test)/nrow(train_test_partition)

conf_matrix <- confusionMatrix(y_hat, train_test_partition$surface)
conf_matrix$overall["Accuracy"]
conf_matrix$table %>% knitr::kable()






fit_model_RF_untuned <- randomForest(surface ~ ., data = x_train_processed)

# predict
y_hat <- predict(fit_model_RF_untuned, select(x_test_processed, -series_id))

x_test_processed_for_submission <- x_test_processed %>% select(series_id) %>% mutate(surface = y_hat)
write_csv(x_test_processed_for_submission, "data/submission_RF_untuned0_02.csv")

```

```{r randomTrees, fig.width=24, fig.height=16}
control <- trainControl(method="repeatedcv", number=10, repeats=2, search="grid")

metric <- "Accuracy"

# mtry <- sqrt(ncol(train_train_partition) - 1)
mtry <- 2:5
tunegrid <- expand.grid(.mtry=mtry,.ntree=c(100, 500, 1000, 1500, 2000, 2500) )

# tunegrid <- expand.grid(.mtry=c(1:10))

customRF 				<- 	list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters 	<- 	data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid 			<- 	function(x, y, len = NULL, search = "grid") {}
customRF$fit 			<- 	function(x, y, wts, param, lev, last, weights, classProbs, ...) randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
customRF$predict 		<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL) predict(modelFit, newdata)
customRF$prob 			<- 	function(modelFit, newdata, preProc = NULL, submodels = NULL)	predict(modelFit, newdata, type = "prob")
customRF$sort 			<- 	function(x) x[order(x[,1]),]
customRF$levels 		<- 	function(x) x$classes


fit_model_rf <- train(surface~., data=train_train_partition, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
print(fit_model_rf)
plot(fit_model_rf)

y_hat <- predict(fit_model_rf, train_test_partition)
y_test <- train_test_partition$surface

# print(sum(y_hat == y_test)/nrow(train_test_partition))

conf_matrix <- confusionMatrix(y_hat, train_test_partition$surface)
conf_matrix$overall["Accuracy"]
conf_matrix$table

# create the model using the whole train data 
# fit_model_rf <- train(surface~., data=x_train_processed, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)

# save the model
# saveRDS(fit_model_rf, file = "models/fit_model_RF_one_vs_one.rds")

# # save the model
# saveRDS(fit_model_rf, file = "models/model_RF_one_vs_one.rds")
# model_soft_pvc <- readRDS("models/model_RF_one_vs_one.rds")
# y_hat <- predict(model_soft_pvc, train_test_partition)
# y_test <- train_test_partition$surface
# print(sum(y_hat == y_test)/nrow(train_test_partition))
```
```{r}
# get just the series_id to be merged later with y_hat
test_series_id <- x_test_processed %>% select(series_id)

# remove series_id from the data set in order to make the predictions
x_test_processed_for_predict <- x_test_processed %>% select(-series_id)

# read the saved model
fit_model_RF <- readRDS("models/fit_model_RF_one_vs_one.rds")

# predict
y_hat <- predict(fit_model_RF, x_test_processed_for_predict)

# produce Kaggle data for submission

submission <- test_series_id %>% mutate(surface = y_hat)
write_csv(submission, "data/submission_02.csv")

```



```{r one-vs-rest}



test_index <- createDataPartition(y = train_df$surface, times = 1, p = 0.5, list = FALSE)
train_partition_ovr <- train_df[-test_index, ]
test_partition_ovr <- train_df[test_index, ]

train_partition_ovr_processed <- train_partition_ovr %>% mutate(surface = as.factor(ifelse(surface == "wood", "wood", "the_rest")))
train_partition_ovr_processed <- pre_process(train_partition_ovr_processed)

test_partition_ovr_processed <- test_partition_ovr %>% mutate(surface = as.factor(ifelse(surface == "wood", "wood", "the_rest")))

fit_model_rf_wood <- train(surface~., data=train_partition_ovr_processed, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
print(fit_model_rf_wood)
plot(fit_model_rf_wood)

y_hat <- predict(fit_model_rf_wood, train_test_partition)
y_test <- train_test_partition$surface

# print(sum(y_hat == y_test)/nrow(train_test_partition))

conf_matrix <- confusionMatrix(y_hat, train_test_partition$surface)
conf_matrix$overall["Accuracy"]
conf_matrix$table

```


```{r warning=FALSE}
fit <- train(surface ~ . , method = "lda", data = train_train_partition)
y_hat <- predict(fit, train_test_partition, type = "raw")
y_test <- train_test_partition$surface

sum(y_hat == y_test)/nrow(train_test_partition)
```




```{r}

fit_knn3 <- knn3(surface ~ ., data = train_train_partition, k = 33)
y_hat <- predict(fit_knn3, train_test_partition, type = "class")
y_test <- train_test_partition$surface

sum(y_hat == y_test)/nrow(train_test_partition)

```

```{r}
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn <- train(x, y, 
                   method = "knn", 
                   tuneGrid = data.frame(k = 10:100),
                   trControl = control)
train_knn

```




## Reference

1. Q2EA: Convert from rotation Quaternions to Euler Angles. Q2EA converts from Quaternions (Q) to Euler Angles (EA) based on D. M. Henderson (1977). Q2EA.Xiao is the algorithm by J. Xiao (2013) for the Princeton Vision Toolkit - included here to allow reproducible research. https://rdrr.io/cran/RSpincalc/man/Q2EA.html

2. Understanding Quaternions. http://www.chrobotics.com/library/understanding-quaternions

3. Understanding Euler Angles. http://www.chrobotics.com/library/understanding-euler-angles

4. Tune Machine Learning Algorithms in R (random forest case study) by Jason Brownlee. https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

5. Classification with more than two classes, from Introduction to Information Retrieval, Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze,, Cambridge University Press 2008  https://nlp.stanford.edu/IR-book/html/htmledition/classification-with-more-than-two-classes-1.html
